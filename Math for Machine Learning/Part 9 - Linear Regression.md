Dalam bab ini, kita akan menerapkan konsep matematika untuk menyelesaikan masalah regresi linear (pencocokan kurva). Tujuan dari regresi adalah untuk menemukan sebuah fungsi $f$ yang memetakan input $x \in \mathbb{R}^D$ ke nilai fungsi yang bersesuaian $f(x) \in \mathbb{R}$. Kita mengasumsikan diberikan satu set data latih yang terdiri dari input $x_n$ dan observasi yang mengandung _noise_ (bising) $$y_n = f(x_n) + \epsilon$$di mana $\epsilon$ adalah variabel acak yang mendeskripsikan _noise_ pengukuran. Dalam bab ini, kita mengasumsikan _noise_ tersebut berdistribusi Gaussian dengan rata-rata nol. Tugas utamanya adalah menemukan fungsi yang tidak hanya memodelkan data latih dengan baik, tetapi juga dapat melakukan generalisasi untuk memprediksi nilai fungsi pada lokasi input baru yang tidak ada dalam data latih. Beberapa masalah utama yang perlu dipecahkan dalam regresi meliputi pemilihan model, penentuan parameter yang baik, penanganan _overfitting_, pemodelan ketidakpastian, dan memahami hubungan antara fungsi kerugian (_loss_) dan asumsi _prior_.
# 9.1 Formulasi Masalah
Karena adanya _noise_ observasi, kita akan menggunakan pendekatan probabilistik. Secara spesifik, kita memodelkan masalah regresi dengan fungsi _likelihood_ (kemungkinan): $p(y | x) = \mathcal{N}(y | f(x), \sigma^2)$. Ini menyiratkan hubungan fungsional $y = f(x) + \epsilon$, di mana $\epsilon \sim \mathcal{N}(0, \sigma^2)$ adalah _noise_ Gaussian dengan rata-rata $0$ dan varians $\sigma^2$. Bab ini berfokus pada model parametrik, di mana parameter $\boldsymbol{\theta}$ muncul secara linear dalam model. Contoh model regresi linear adalah: $$p(y | x, \boldsymbol{\theta}) = \mathcal{N}(y | x^\top\boldsymbol{\theta}, \sigma^2) \iff y = x^\top\boldsymbol{\theta} + \epsilon$$Penting untuk dicatat bahwa istilah "regresi linear" merujuk pada model yang linear dalam parameter, bukan harus linear dalam input $x$. Hal ini memungkinkan kita untuk menerapkan transformasi non-linear $\boldsymbol{\phi}(x)$ pada input dan tetap berada dalam kerangka regresi linear.

# 9.2 Estimasi Parameter

Diberikan sebuah set data latih $\mathcal{D} := {(x_1, y_1), \dots, (x_N, y_N)}$, kita bertujuan untuk menemukan parameter optimal $\boldsymbol{\theta}^*$. Karena setiap observasi $y_n$ independen secara kondisional, _likelihood_ dari keseluruhan data dapat difaktorkan menjadi produk dari _likelihood_ individual: $$p(\mathbf{Y} | x, \boldsymbol{\theta}) = \prod_{n=1}^{N} p(y_n | x_n, \boldsymbol{\theta}) = \prod_{n=1}^{N} \mathcal{N}(y_n | x_n^\top\boldsymbol{\theta}, \sigma^2)$$
## 9.2.1 Estimasi Kemungkinan Maksimum (_Maximum Likelihood Estimation_)
_Maximum Likelihood Estimation_ (MLE) adalah pendekatan untuk menemukan parameter $\boldsymbol{\theta}_{\text{ML}}$ yang memaksimalkan fungsi _likelihood_ di atas. Dalam praktiknya, kita sering kali memaksimalkan logaritma dari _likelihood_ (log-_likelihood_) karena transformasi logaritmik tidak mengubah lokasi maksimum dan menyederhanakan perhitungan turunan dengan mengubah produk menjadi penjumlahan. Memaksimalkan log-_likelihood_ setara dengan meminimalkan _negative log-likelihood_, yang untuk model regresi linear dengan _noise_ Gaussian, dapat ditulis sebagai (mengabaikan konstanta): $$L(\boldsymbol{\theta}) := \frac{1}{2\sigma^2} \sum_{n=1}^{N} (y_n - x_n^\top\boldsymbol{\theta})^2 = \frac{1}{2\sigma^2} |\mathbf{y} - x\boldsymbol{\theta}|^2$$Di sini, $x$ adalah matriks desain di mana setiap baris adalah $x_n^\top$, dan $\mathbf{y}$ adalah vektor yang berisi semua target $y_n$. Karena fungsi ini kuadratik terhadap $\boldsymbol{\theta}$, solusi global unik dapat ditemukan dengan mengatur gradiennya menjadi nol. Solusi untuk $\boldsymbol{\theta}_{\text{ML}}$ adalah: $$\boldsymbol{\theta}_{\text{ML}} = (x^\top x)^{-1}x^\top\mathbf{y}$$Jika kita menggunakan fitur non-linear $\boldsymbol{\phi}(x)$, matriks desain $x$ digantikan oleh matriks fitur $\mathbf{\Phi}$, dan solusinya menjadi $$\boldsymbol{\theta}_{\text{ML}} = (\mathbf{\Phi}^\top\mathbf{\Phi})^{-1}\mathbf{\Phi}^\top\mathbf{y}$$Varians _noise_ juga dapat diestimasi menggunakan MLE, menghasilkan $$\sigma^2_{\text{ML}} = \frac{1}{N}\sum_{n=1}^N (y_n - \boldsymbol{\phi}(x_n)^\top\boldsymbol{\theta})^2$$
## 9.2.2 _Overfitting_ dalam Regresi Linear
_Overfitting_ terjadi ketika model terlalu fleksibel dan "terlalu cocok" dengan data latih, sehingga gagal menggeneralisasi dengan baik pada data yang tidak terlihat. Contohnya, pada regresi polinomial, penggunaan derajat polinomial yang sangat tinggi ($M \geq N-1$) akan membuat fungsi melewati setiap titik data latih tetapi berosilasi secara liar di antara titik-titik tersebut. Kualitas model sering dievaluasi menggunakan _Root Mean Square Error_ (RMSE) pada data latih dan data uji terpisah. Seiring dengan meningkatnya kompleksitas model (misalnya, derajat polinomial $M$), _training error_ akan terus menurun, tetapi _test error_ pada suatu titik akan mulai meningkat, yang menandakan terjadinya _overfitting_.
## 9.2.3 Estimasi _Maximum A Posteriori_ (MAP)
Untuk mengatasi _overfitting_, kita dapat menempatkan distribusi _prior_ $p(\boldsymbol{\theta})$ pada parameter, yang mencerminkan keyakinan kita tentang nilai parameter yang masuk akal sebelum melihat data. Alih-alih memaksimalkan _likelihood_, estimasi MAP memaksimalkan distribusi _posterior_ $p(\boldsymbol{\theta} | x, \mathbf{Y})$, yang menurut Teorema Bayes sebanding dengan perkalian antara _likelihood_ dan _prior_: $$p(\boldsymbol{\theta} | x, \mathbf{Y}) \propto p(\mathbf{Y} | x, \boldsymbol{\theta}) p(\boldsymbol{\theta})$$Dengan asumsi _prior_ Gaussian, $p(\boldsymbol{\theta}) = \mathcal{N}(\mathbf{0}, b^2\mathbf{I})$, estimasi MAP $\boldsymbol{\theta}_{\text{MAP}}$ ditemukan dengan meminimalkan _negative log-posterior_. Solusinya adalah: $$\boldsymbol{\theta}_{\text{MAP}} = \left(\mathbf{\Phi}^\top\mathbf{\Phi} + \frac{\sigma^2}{b^2}\mathbf{I}\right)^{-1}\mathbf{\Phi}^\top\mathbf{y}$$Penambahan suku $\frac{\sigma^2}{b^2}\mathbf{I}$ membantu mencegah _overfitting_ dengan "menghukum" nilai parameter yang besar.

## 9.2.4 Estimasi MAP sebagai Regularisasi
Pendekatan MAP sangat terkait dengan regularisasi. Meminimalkan _negative log-posterior_ dengan _prior_ Gaussian setara dengan meminimalkan fungsi kerugian _regularized least squares_: $$|\mathbf{y} - \mathbf{\Phi}\boldsymbol{\theta}|^2 + \lambda |\boldsymbol{\theta}|_2^2$$Suku pertama adalah _data-fit term_ (berasal dari _likelihood_), dan suku kedua adalah _regularizer_ (berasal dari _prior_). Dalam kasus ini, parameter regularisasi $\lambda$ berhubungan langsung dengan varians dari _likelihood_ dan _prior_ ($\lambda = \sigma^2 / b^2$).

# 9.3 Regresi Linear Bayesian
Berbeda dengan MLE dan MAP yang menghasilkan estimasi titik tunggal untuk parameter, regresi linear Bayesian menghitung distribusi _posterior_ penuh atas parameter, $p(\boldsymbol{\theta} | x, \mathbf{Y})$. Dengan menggunakan _prior_ Gaussian $p(\boldsymbol{\theta}) = \mathcal{N}(\mathbf{m}_0, \mathbf{S}_0)$ dan _likelihood_ Gaussian (model konjugat), distribusi _posterior_ yang dihasilkan juga Gaussian: $$p(\boldsymbol{\theta} | x, \mathbf{Y}) = \mathcal{N}(\boldsymbol{\theta} | \mathbf{m}_N, \mathbf{S}_N)$$dengan _mean_ dan kovarians _posterior_ diberikan oleh: $$\begin{align}
\mathbf{S}_N = (\mathbf{S}_0^{-1} + \sigma^{-2}\mathbf{\Phi}^\top\mathbf{\Phi})^{-1} \\
\mathbf{m}_N = \mathbf{S}_N(\mathbf{S}_0^{-1}\mathbf{m}_0 + \sigma^{-2}\mathbf{\Phi}^\top\mathbf{y})
\end{align}$$Prediksi untuk input baru $x_*$ dibuat dengan melakukan marginalisasi (rata-rata) atas semua kemungkinan parameter sesuai dengan distribusi _posterior_ mereka: $$
\begin{align}
p(y_* | \mathcal{X}, \mathcal{Y}, x_*) &  = \int p(y_* | x_*, \boldsymbol{\theta}) p(\boldsymbol{\theta} | \mathcal{X}, \mathcal{Y}) d\boldsymbol{\theta} \\
 & = \mathcal{N}(y_* | \boldsymbol{\phi} ^\top (x_{*}){\mathbf{m}_N}, \boldsymbol{\phi} ^\top (x_{*}){\mathbf{S}_N}\boldsymbol{\phi}(x_*) + \sigma^2)
\end{align}
$$Hasilnya adalah distribusi prediktif yang juga Gaussian. Varians prediktif ini mencakup dua sumber ketidakpastian: ketidakpastian dari _noise_ pengukuran ($\sigma^2$) dan ketidakpastian dari parameter ($\boldsymbol{\phi}(x_{*})^\top\mathbf{S}_N\boldsymbol{\phi}(x_{*})$).
# 9.4 Kemungkinan Maksimum sebagai Proyeksi Ortogonal
Estimasi _maximum likelihood_ memiliki interpretasi geometris yang kuat. Solusi MLE untuk regresi linear, $x\boldsymbol{\theta}_{\text{ML}}$, secara efektif merupakan proyeksi ortogonal dari vektor target observasi $\mathbf{y}$ ke subruang yang direntang oleh kolom-kolom matriks desain $x$ (atau matriks fitur $\mathbf{\Phi}$). Dengan kata lain, MLE menemukan vektor dalam subruang model yang memiliki jarak kuadrat terkecil (paling "dekat") dengan data observasi $\mathbf{y}$.