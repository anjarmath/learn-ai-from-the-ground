Estimasi kepadatan bertujuan untuk merepresentasikan data secara ringkas menggunakan sebuah fungsi kepadatan dari keluarga parametrik, seperti distribusi Gaussian. Namun, distribusi tunggal seperti Gaussian sering kali memiliki kemampuan pemodelan yang terbatas dan tidak dapat merepresentasikan data yang kompleks secara memadai, misalnya data dengan beberapa "gugus" (multimodal). _Mixture models_ menawarkan solusi yang lebih ekspresif dengan merepresentasikan sebuah distribusi $p(x)$ sebagai kombinasi cembung dari $K$ distribusi dasar (komponen) yang sederhana: $$ p(x) = \sum_{k=1}^{K} \pi_k p_k(x) $$ dengan kendala pada bobot campuran $\pi_k$ yaitu $0 \le \pi_k \le 1$ dan $\sum_{k=1}^{K} \pi_k = 1$. Bab ini berfokus pada _Gaussian Mixture Models_ (GMM), di mana distribusi dasarnya adalah Gaussian. Karena tidak ada solusi bentuk-tertutup untuk estimasi parameter GMM, kita akan menggunakan skema iteratif.
#### 11.1 Gaussian Mixture Model
Sebuah _Gaussian Mixture Model_ (GMM) adalah sebuah model kepadatan di mana kita menggabungkan sejumlah terbatas $K$ distribusi Gaussian $\mathcal{N}(x|\mu_k, \Sigma_k)$ sehingga: $$ p(x|\theta) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x|\mu_k, \Sigma_k) $$di mana $\theta := \{\mu_k, \Sigma_k, \pi_k : k=1, \dots, K\}$ adalah kumpulan semua parameter model. Kombinasi cembung ini memberikan fleksibilitas yang jauh lebih besar untuk memodelkan kepadatan yang kompleks dibandingkan dengan distribusi Gaussian tunggal. Contohnya adalah pada gaussian mixture berikut:
![[Pasted image 20250930133813.png]]
$$
p(x|\theta)=0.5\mathcal{N}\left( x|-2, \frac{1}{2} \right)+0.2\mathcal{N}(x|1,2)+0.3\mathcal{N}(x|4|1)
$$
#### 11.2 Pembelajaran Parameter melalui Maximum Likelihood
Dengan asumsi kita memiliki set data $X = {x_1, \dots, x_N}$ yang ditarik dari distribusi yang tidak diketahui, tujuan kita adalah mencari parameter GMM yang dapat merepresentasikan distribusi ini dengan baik. Kita menggunakan estimasi _maximum likelihood_. _Log-likelihood_ dari data didefinisikan sebagai: $$ \log p(X|\theta) = \sum_{n=1}^{N} \log \left( \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k) \right) $$Tantangan utamanya adalah keberadaan logaritma di luar penjumlahan, yang menghalangi kita untuk mendapatkan solusi bentuk-tertutup. Sebagai gantinya, kita akan mencari kondisi optimalitas dengan mengatur gradien dari _log-likelihood_ terhadap setiap parameter menjadi nol.
##### 11.2.1 Responsibilities
Kita mendefinisikan sebuah kuantitas sentral yang disebut _responsibility_ (tanggung jawab) dari komponen campuran ke-$k$ untuk titik data ke-$n$ sebagai: $$ r_{nk} := \frac{\pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_n|\mu_j, \Sigma_j)} $$ Nilai $r_{nk}$ ini dapat diinterpretasikan sebagai probabilitas bahwa titik data $x_n$ telah dihasilkan oleh komponen campuran ke-$k$. Ini adalah bentuk "penugasan lunak" (_soft assignment_) dari setiap titik data ke $K$ komponen, di mana untuk setiap $n$, $\sum_k r_{nk} = 1$.
##### 11.2.2 Memperbarui Rata-rata (Means)
Dengan mengatur turunan parsial dari _log-likelihood_ terhadap $\mu_k$ menjadi nol, kita mendapatkan aturan pembaruan untuk rata-rata: $$ \mu_k^{\text{new}} = \frac{\sum_{n=1}^{N} r_{nk} x_n}{\sum_{n=1}^{N} r_{nk}} $$ Pembaruan ini merupakan rata-rata tertimbang dari titik-titik data, di mana bobotnya adalah _responsibility_. Kita mendefinisikan $N_k := \sum_{n=1}^{N} r_{nk}$ sebagai total _responsibility_ dari komponen ke-$k$.
##### 11.2.3 Memperbarui Kovariansi
Dengan cara yang sama, aturan pembaruan untuk matriks kovariansi $\Sigma_k$ diperoleh dengan mengatur turunan parsial _log-likelihood_ menjadi nol: $$ \Sigma_k^{\text{new}} = \frac{1}{N_k} \sum_{n=1}^{N} r_{nk} (x_n - \mu_k)(x_n - \mu_k)^\top $$ Ini dapat diartikan sebagai estimasi kovariansi tertimbang untuk setiap komponen.
##### 11.2.4 Memperbarui Bobot Campuran
Untuk memperbarui bobot campuran $\pi_k$ dengan kendala $\sum_k \pi_k = 1$, kita menggunakan Pengali Lagrange. Hasilnya adalah aturan pembaruan: $$ \pi_k^{\text{new}} = \frac{N_k}{N} $$ Bobot baru untuk komponen ke-$k$ adalah rasio dari total _responsibility_-nya terhadap jumlah total titik data.
#### 11.3 Algoritma EM
Karena persamaan pembaruan untuk semua parameter saling bergantung satu sama lain melalui _responsibilities_, kita tidak dapat memperoleh solusi bentuk-tertutup. Namun, ini mengarah pada skema iteratif yang sederhana yang disebut algoritma _Expectation-Maximization_ (EM). Algoritma EM untuk GMM bergantian antara dua langkah berikut hingga konvergensi:

1. **Langkah-E (Ekspektasi):** Evaluasi _responsibilities_ $r_{nk}$ menggunakan nilai parameter saat ini ${\pi_k, \mu_k, \Sigma_k}$.
2. **Langkah-M (Maksimisasi):** Estimasi ulang parameter ${\pi_k, \mu_k, \Sigma_k}$ menggunakan _responsibilities_ yang baru dihitung dari Langkah-E dengan persamaan pembaruan yang telah diturunkan. Setiap iterasi dari algoritma EM dijamin akan meningkatkan (atau tidak menurunkan) nilai _log-likelihood_.
#### 11.4 Perspektif Variabel Laten
GMM dapat dipandang sebagai model dengan variabel laten diskrit, yang memberikan landasan teoretis yang lebih kokoh untuk algoritma EM. Kita memperkenalkan variabel laten biner $z_k \in {0, 1}$ yang mengindikasikan komponen mana yang menghasilkan sebuah titik data. Vektor $z = [z_1, \dots, z_K]^\top$ adalah representasi _one-hot_, dengan hanya satu entri bernilai 1.
- Distribusi prior pada variabel laten ini adalah $p(z_k=1) = \pi_k$.
- Distribusi kondisional dari data adalah $p(x | z_k=1) = \mathcal{N}(x|\mu_k, \Sigma_k)$. Untuk mendapatkan _likelihood_ $p(x|\theta)$, kita melakukan marginalisasi terhadap variabel laten $z$: $$ p(x|\theta) = \sum_{z} p(x|\theta, z) p(z|\theta) = \sum_{k=1}^{K} p(x|\theta, z_k=1) p(z_k=1|\theta) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x|\mu_k, \Sigma_k) $$ Persamaan ini sama persis dengan definisi GMM di awal. Dari perspektif ini, _responsibility_ $r_{nk}$ ternyata adalah probabilitas posterior dari variabel laten, yaitu $p(z_{nk}=1|x_n)$. Ini memberikan interpretasi matematis yang sahih untuk _responsibilities_.